# Learnings - robots-sitemap-fix

## Conventions

## Patterns

## Gotchas

## Next.js Sitemap Implementation (Task 2)

### Patterns
- `MetadataRoute.Sitemap` type returns array of `{ url: string, lastModified?: Date }`
- Async sitemap function must be default export with type annotation
- ISR revalidation set via `export const revalidate = 3600` (1 hour)
- Strapi API response format: `{ data: Post[] }` - extract with `.then((data) => data.data)`
- Try/catch fallback to static routes only when API unavailable - graceful degradation

### Gotchas
- Fetch pattern from blog/page.tsx: `populate=%2A&sort[1]=createdAt%3Adesc` (URL-encoded)
- Post.updatedAt used for lastModified (more accurate than createdAt)
- Static routes use `new Date()` for lastModified; dynamic routes use `new Date(post.updatedAt)`
- Site URL prefix: `https://dimitristrechas.com` (hardcoded, not env var)

### Verification
- sitemap.xml endpoint auto-generated by Next.js at `/sitemap.xml`
- XML validates: includes `<urlset>` root, `<loc>` entries for all routes
- Dynamic blog posts fetched from Strapi and included in output
- Error handling tested conceptually - returns static routes if Strapi fails

## AI Crawler Blocking with Next.js robots.ts

### Implementation Details
- Used MetadataRoute.Robots type from Next.js for type safety
- Each AI crawler (GPTBot, ChatGPT-User, ClaudeBot, anthropic-ai, Claude-Web, Google-Extended, CCBot) has individual rule with `disallow: '/'`
- Catch-all rule `user-agent: '*'` with `allow: '/'` allows standard crawlers
- Sitemap reference: `https://dimitristrechas.com/sitemap.xml`

### RFC 9309 Compliance
- Removed invalid `Content-Signal` directive (not RFC-compliant)
- Valid directives per RFC 9309: user-agent, allow, disallow, sitemap
- Added comment explaining this change for future reference

### Verification
- `npm run build` succeeded - robots.ts compiles cleanly
- `curl http://localhost:3001/robots.txt` returns valid robots.txt format
- All 7 AI crawlers blocked with `Disallow: /`
- `Content-Signal` directive absent from output (correct)
- Sitemap properly referenced

### Port Note
Dev server runs on port 3001 (as per AGENTS.md and npm config)

## Final Verification [2026-01-27]

### robots.txt Output (Local Dev)
```
User-Agent: GPTBot
Disallow: /

User-Agent: ChatGPT-User
Disallow: /

User-Agent: ClaudeBot
Disallow: /

User-Agent: anthropic-ai
Disallow: /

User-Agent: Claude-Web
Disallow: /

User-Agent: Google-Extended
Disallow: /

User-Agent: CCBot
Disallow: /

User-Agent: *
Allow: /

Sitemap: https://dimitristrechas.com/sitemap.xml
```

✅ No `Content-Signal` directive present
✅ All 7 AI crawlers properly blocked
✅ Catch-all allows other crawlers
✅ Sitemap reference included

### sitemap.xml Output (Local Dev)
```xml
<?xml version="1.0" encoding="UTF-8"?>
<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">
  <url><loc>https://dimitristrechas.com</loc></url>
  <url><loc>https://dimitristrechas.com/about</loc></url>
  <url><loc>https://dimitristrechas.com/contact</loc></url>
  <url><loc>https://dimitristrechas.com/blog</loc></url>
  <url><loc>https://dimitristrechas.com/blog/sample-post-c98186b9</loc></url>
  <!-- Additional blog posts dynamically included -->
</urlset>
```

✅ Valid XML with proper namespace
✅ All static routes included
✅ Dynamic blog posts from Strapi included
✅ ISR revalidation working (1 hour)

### Remaining Manual Steps
⚠️ Deploy to production and run Lighthouse SEO audit to confirm error resolution

## Middleware Implementation for robots.txt (Task 3) [2026-01-27]

### Implementation Pattern
- Next.js middleware intercepts requests at edge before Cloudflare can inject headers
- File location: project root `middleware.ts` (same level as `app/`, `package.json`)
- Matcher config: `export const config = { matcher: "/robots.txt" }` for surgical targeting

### Middleware Structure
```typescript
import { NextRequest, NextResponse } from "next/server";

export function middleware(request: NextRequest) {
  if (request.nextUrl.pathname === "/robots.txt") {
    const content = `<exact robots.txt content>`;
    return new NextResponse(content, {
      status: 200,
      headers: {
        "Content-Type": "text/plain",
        "Cache-Control": "public, max-age=3600",
      },
    });
  }
}

export const config = {
  matcher: "/robots.txt",
};
```

### Headers Set
- `Content-Type: text/plain` - RFC 9309 compliance
- `Cache-Control: public, max-age=3600` - 1-hour browser cache

### Content Verification
Middleware serves exact same content as `app/robots.ts` export:
- 7 AI crawlers blocked (GPTBot, ChatGPT-User, ClaudeBot, anthropic-ai, Claude-Web, Google-Extended, CCBot)
- Catch-all `User-agent: *` with `Allow: /`
- Sitemap reference: `https://dimitristrechas.com/sitemap.xml`

### Bypass Mechanism
- Cloudflare injections occur AFTER Next.js middleware layer
- By intercepting at middleware level, response bypasses Cloudflare's Content-Signal injection
- No `Content-Signal` directive in middleware response

### Verification Results
```
curl -i http://localhost:3001/robots.txt

HTTP/1.1 200 OK
cache-control: public, max-age=3600
content-type: text/plain
...

User-agent: GPTBot
Disallow: /
...
User-agent: *
Allow: /

Sitemap: https://dimitristrechas.com/sitemap.xml
```

✅ Middleware successfully intercepts `/robots.txt`
✅ No Cloudflare Content-Signal injection in response
✅ All 7 AI crawlers blocked
✅ RFC 9309 compliant
✅ Headers correctly set

### Trade-offs
- Adds Edge runtime overhead (minimal)
- Content duplicated from `app/robots.ts` (maintainability trade-off for reliability)
- Middleware runs on every request but matcher limits to `/robots.txt` only

## Next.js 16 proxy.ts Convention [2026-01-27]

### Migration from middleware.ts
- Next.js 16 deprecated `middleware.ts` file convention
- New convention: `proxy.ts` (same directory structure)
- Function export renamed: `export function middleware` → `export function proxy`
- All other code remains identical (NextRequest, NextResponse, config.matcher)

### Deprecation Warning
```
⚠ The "middleware" file convention is deprecated. Please use "proxy" instead.
```

### Migration Steps
1. Rename file: `middleware.ts` → `proxy.ts`
2. Rename export: `export function middleware(...)` → `export function proxy(...)`
3. Keep all other code unchanged (imports, logic, config)

### Verification
- Build completes with NO deprecation warning
- robots.txt interception still works identically
- `curl http://localhost:3001/robots.txt` returns RFC 9309 compliant content
- Build output shows: `ƒ Proxy (Middleware)` confirming interception active

### Pattern Reference
```typescript
// proxy.ts
import { NextRequest, NextResponse } from "next/server";

export function proxy(request: NextRequest) {
  // ... same logic as before
}

export const config = {
  matcher: "/robots.txt",
};
```
